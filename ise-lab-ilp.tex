%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ISE Lab -- Topic
% Giovanni Ciatto
% Alma Mater Studiorum - Universit√† di Bologna
% mailto:giovanni.ciatto@unibo.it
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{default}}
%
\documentclass[presentation]{beamer}\mode<presentation>{\usetheme{AMSBolognaFC}}
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{AMSBolognaFC}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{ise-lab-common}
\usepackage{ise-lab-ilp}
% version
\newcommand{\versionmajor}{1}
\newcommand{\versionminor}{0}
\newcommand{\versionpatch}{1}
\newcommand{\version}{\versionmajor.\versionminor.\versionpatch}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[\currentLab{} -- Symbolic ML \& ILP]{
    Symbolic Machine Learning and Inductive Logic Programming
}
%
\subtitle{\courseName{} / Module \moduleN{} (\courseAcronym)}
%
\author[\sspeaker{\gcShort}]{\speaker{\gcFull} \\ \gcEmail}
%
\institute[\disiShort, \uniboShort]{\disi{} (\disiShort)\\\unibo}
%
\date[A.Y. \academicYear{} (v.\ \version)]{Academic Year \academicYear{}\\(version \version)}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%/////////
\frame{\titlepage}
%/////////

%%===============================================================================
\section*{Outline}
%%===============================================================================
%
%%/////////
\frame[c]{\tableofcontents[hideallsubsections]}
%%/////////

%===============================================================================
\section{Machine Learning}
%===============================================================================

\begin{frame}[allowframebreaks]
\frametitle{Machine Learning (ML)}
    \begin{block}{A famous definition of ML from \cite{Mitchell1997}:}\itshape
        A computer program is said to learn from \alert{experience $E$} with respect to some class of \alert{tasks $T$} and \alert{performance $P$} if its performance at tasks in $T$, as measured by $P$, improves with experience $E$
    \end{block}

    \begin{alertblock}{Very \textbf{wide} definition, not specifying:}
        %
        \begin{itemize}
            \item what are the possible tasks?
            \item how performance is measured for a given task?
            \item what is experience?
            \item how / when experience should be provided to programs?
            \item what parts of the program are supposed to change via learning?
            \item under which form learnt information is represented?
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Many ways to categorise ML}
    \begin{block}{Supervised, unsupervised, reinforcement learning}
        \begin{itemize}
            \item \textbf{Supervised} learning:
            %
            \begin{itemize}
                \item[$T$] $=$ approximation for unknown \alert{relation}  
                \item[$E$] $=$ a statistically-relevant \alert{sample} of that relation
                \item[$P$] $=$ a \alert{quality measure} for the approximation
            \end{itemize}

            \item \textbf{Unsupervised} learning:
            %
            \begin{itemize}
                \item[$E$] $=$ \alert{set} of \alert{homogeneous} data 
                \item[$T$] $=$ \alert{partitioning} of data into multiple sets
                \item[$P$] $=$ \alert{optimality criterion} intensionally describing the partitioning 
            \end{itemize} 

            \item \textbf{Reinforcement} learning:
            %
            \begin{itemize}
                \item[$T$] $=$ letting agents choose actions in states  
                \item[$E$] $=$ \alert{reward} received for achieving a goal 
                \item[$P$] $=$ \alert{optimality criterion} for the policy
            \end{itemize} 
        \end{itemize}
    \end{block}

    \begin{block}{Symbolic vs. sub-symbolic learning}
        \begin{itemize}
            \item depends on whether the learnt information can be \alert{symbolically} represented or not
        \end{itemize}
    \end{block}

    \begin{block}{On-line vs. off-line learning}
        \begin{description}
            \item[Off-line] $\approx$ learning occurs \alert{before} operation
            \item[On-line] $\approx$ learning occurs \alert{during} operation
        \end{description}
    \end{block}
    
    \begin{center}
        \includegraphics[width=.7\linewidth]{figures/ml-taxonomy.pdf}
    \end{center}
\end{frame}

%===============================================================================
\section{Symbolic Machine Learning}
%===============================================================================

\begin{frame}[allowframebreaks]
\frametitle{Symbolic Supervised Learning}

    \begin{block}{Insights}
        \begin{itemize}
            \item Aims at learning a \alert{relation from examples} (supervised)
            \item The learnt relation is a \alert{logic program} (symbolic)
            \item May learn \alert{intensional} representations too
            \item Has analogies and overlaps with
            %
            \begin{itemize}
                \item (automatic) program synthesis
                \item probabilistic logic programming\ccite{Ng1992}
            \end{itemize}
            \item Two major (partially overlapping) branches:
            %
            \begin{itemize}
                \item inductive logic programming\ccite{Muggleton1991} (ILP)
                \item statistical relational learning\ccite{DeRaedt2010} (SRL)
                %
                \begin{itemize}
                    \item[aka] \emph{probabilistic} inductive logic programming\ccite{RaedtK04}
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Formulation \cite{DeRaedt2010}}
        Given
        %
        \begin{itemize}
            \item a \alert{background knowledge} $B$, i.e. a theory of known clauses
            \item a set of \alert{positive examples} $E^+$, i.e. a theory of ground, \alert{true} facts
            %
            % \begin{itemize}
            %     \item known to be 
            % \end{itemize}
            \item a set of \alert{negative examples} $E^-$, i.e. a theory of ground, \alert{false} facts
            %
            % \begin{itemize}
            %     \item known to be \alert{false}
            % \end{itemize}
        \end{itemize}
        %
        the goal is to 
        %
        \begin{itemize}
            \item \alert{(parameters learning)} tune a \alert{known} (yet \alert{parametric}) theory
            %
            \begin{itemize}
                \item possibly leveraging on clauses in $B$
                \item maximising the probability of $E^+$ being true
                \item minimising the probability of $E^-$ being false
            \end{itemize}
            %
            \item \alert{(structure learning)} induce a theory \alert{from scratch}
            %
            \begin{itemize}
                \item possibly leveraging on clauses in $B$
                \item from which all facts in $E^+$ and no facts in $E^-$ can be proved
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[allowframebreaks]
    \frametitle{Examples}

    Parameters learning:
    %
    \prologimport{listings/slr-example.pl}

    \begin{exampleblock}{Here the goal is tuning the \textbf{probabilities}}
        \begin{itemize}
            \item $\alert{p} = 75\%$, \qquad $\alert{q} = 25\%$
        \end{itemize}
    \end{exampleblock}

    \framebreak

    Structure learning:
    %
    \prologimport{listings/ilp-example.pl}
    %
    \begin{exampleblock}{Here the goal is \textbf{inducing} the theory}
        \begin{itemize}
            \item \prolog{grand_parent(X, Y) :- parent(X, Z), parent(Z, Y).}
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}[allowframebreaks]
    \frametitle{Learning as Search}

    Both in structure and parameter learning:
    %
    \bigskip
    %
    \begin{itemize}
        \item the goals is \alert{finding} some \alert{optimal} theory $H^*$\ldots
        %
        \begin{itemize}
            \item which implies \alert{searching}
            \item following an \alert{optimility criterion}
        \end{itemize}

        \bigskip

        \item \ldots among a set of admissible hypotheses $\mathcal{H}$ (a.k.a. \alert{hypotheses space})
        %
        \begin{itemize}
            \item containing of admissible \alert{logic theories}
        \end{itemize}

        \bigskip

        \item under \alert{given assumptions} on the admissible shapes all $H \in \mathcal{H}$ 
    \end{itemize}

    \framebreak

    \begin{block}{The set of all possible logic theories is \textbf{infinite}}
        E.g., in the case of \alert{Horn logic}, many sources of \emph{infiniteness}:
        %
        \begin{itemize}
            \item infinitely many terms (as soon as $n$-ary functors are used)
            \item infinitely many ways to apply $m$-ary predications to those terms
            \item infinitely many ways to combine predicates into clauses
            \item[$\vdots$]
            %
            \medskip
            %
            \item[$\rightarrow$] need to $\begin{cases}
                \text{``reduce the size of'' (a.k.a. sample) of } \mathcal{H}
                \\
                \text{engineer a smart exploration strategy}
            \end{cases}$
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[allowframebreaks]
    \frametitle{The Notion of Linguistic Bias}

    \begin{block}{Informal definition}
        A set of constraints on the form of the theories in the hypotheses space
    \end{block}

    \begin{exampleblock}{Example of linguistic bias for \textbf{parameter} learning}
        \begin{itemize}
            \item e.g. \alert{logic programs with annotated disjunctions}\ccite{Vennekens2004} (LPAD), where each clause is of the form:
            %
            \[ \alert{p_1} :: h_1(\bar{X}) \alert{\vee} \ldots \alert{\vee} \alert{p_m} :: h_m(\bar{X}) \leftarrow f(\bar{X}) \wedge f'(\bar{X}') \wedge f''(\bar{X}''),\ \ldots \]
            %
            where $\frac{p_i}{\sum_j^{m} p_j}$ is the probability of predicate $h_i(\bar{X})$ being true
        \end{itemize}
    \end{exampleblock}

    \begin{exampleblock}{Examples of linguistic bias for \textbf{structure} learning}
        \begin{itemize}
            \item[e.g.] no functors, only constants + predications of arity $\leq ~ m$
            \item[e.g.] mode declaration\ccite{Muggleton95} (i.e. predicates names and types)
            \item[e.g.] meta rules (i.e. $2^{nd}$-order rules with $2^{nd}$-order variables):
            %
            \[ \mathbf{P}(A,\ B) \leftarrow \mathbf{Q}(A,\ C) \wedge \mathbf{R}(C,\ B) \]
            %
            where $\mathbf{P}, \mathbf{Q}, \mathbf{R}$ range over predicate symbols
        \end{itemize}
    \end{exampleblock}
\end{frame}

%===============================================================================
\section{Inductive Logic Programming}
%===============================================================================

\begin{frame}{Formulation}
    Let
    %
    \begin{itemize}
        \item[$B$] be the \alert{background knowledge}, i.e. a theory of known clauses
        \item[$E^+$] be a set of \alert{positive examples}, i.e. a theory of ground, \alert{true} facts
        \item[$E^-$] be a set of \alert{negative examples},  i.e. a theory of ground, \alert{false} facts
        \item[$C$] be a set of constraints (determining the \alert{linguistic bias})
        \item[$\mathcal{H}$] be the set of theories (acting as \alert{hypotheses space})
        %
        \begin{itemize}
            \item spawned by the constants, functors, and predicate symbols in $B \cup E^+ \cup E^-$, coherently w.r.t. the constraints in $C$
        \end{itemize}
    \end{itemize}
    %
    then ILP is the problem of computing the \emph{minimal} $H^*$ such that:
    %
    \[ H^* = \argmax{H \in \mathcal{H} ~ \text{s.t.} ~ \rho(E^-, H \cup B) = 0}{\rho(E^+, H \cup B)} \]
    %
    where \alert{$\rho(E, K)$} the \alert{coverage} function defined as follows:
    %
    \[ 
        \rho(e, K) = \begin{cases}
            1 & \text{if} ~ K \models e
            \\
            0 & \text{otherwise}
        \end{cases}
        \quad \text{and} \quad
        \rho(E, K) = \sum_{e \in E} \rho(e, K)
    \]
\end{frame}

\begin{frame}[allowframebreaks]
    \frametitle{Search strategies for ILP}
    \begin{block}{Top-down}
        \begin{enumerate}
            \item start from a \alert{general} form 
            \item \alert{specialize} it a bit
            \item modify it a bit
            \item repeat until sufficiently specialised
        \end{enumerate}
        \medskip
        Risks: \alert{over-fitting}, search space explosion
    \end{block}

    \begin{exampleblock}{Example of specialization}
        \begin{center}
            $\mathbf{P}(A,\ B) \leftarrow \mathbf{Q}(A,\ C) \wedge \mathbf{R}(C,\ B)$

            \qquad \qquad $\Downarrow$ { \tiny $[\mathbf{P} = \textit{grand\_parent}, \mathbf{Q} = \mathbf{R} = \textit{parent}]$}

            $\textit{grand\_parent}(A,\ B) \leftarrow \textit{parent}(A,\ C) \wedge \textit{parent}(C,\ B)$
        \end{center}
    \end{exampleblock}

    \begin{block}{Bottom-up}
        \begin{enumerate}
            \item start from a very \alert{specific} form
            \item \alert{generalise} it a bit
            \item modify it a bit
            \item repeat until sufficiently general
        \end{enumerate}
        \medskip
        Risks: inducing useless clauses 
    \end{block}

    \begin{exampleblock}{Example of generalization}
        \begin{center}\footnotesize
            $\textit{grand\_parent}(\texttt{abraham},\ \texttt{bart}) \leftarrow \textit{parent}(\texttt{abraham},\ \texttt{homer}) \wedge \textit{parent}(\texttt{homer},\ \texttt{bart})$
            
            \normalsize
            \qquad \qquad $\Downarrow$ { \tiny $[\texttt{abraham} = A, \texttt{bart} = B, \texttt{homer} = C]$}

            $\textit{grand\_parent}(A,\ B) \leftarrow \textit{parent}(A,\ C) \wedge \textit{parent}(C,\ B)$
        \end{center}
    \end{exampleblock}

\end{frame}

\begin{frame}[allowframebreaks]
    \frametitle{Approaches and Mechanisms for ILP\ccite{CropperD2020}}

    \begin{block}{Relative Least-General Generalization (RLGG) \cite{Buntine88}}
        \begin{itemize}
            \item Exploration strategy: bottom-up
            \item Basic mechanism: \alert{least-general generalization}\ccite{Plotkin1971}
            \item Example algorithms: Golem\ccite{MuggletonF1990}
        \end{itemize}
    \end{block}

    \begin{block}{Inverse Entailment \cite{Muggleton95}}
        \begin{itemize}
            \item Exploration strategy: top-down + bottom-up
            \item Basic mechanism: \alert{bottom clause construction}\ccite{Muggleton95}
            \item Example algorithms: Progol\ccite{Muggleton95}, Aleph\ccite{aleph}
        \end{itemize}
    \end{block}

    \begin{block}{Bottom Clause Propositionalisation \cite{Franca2014}}
        \begin{itemize}
            \item Exploration strategy: bottom-up
            \item Basic mechanism: \alert{bottom clause propositionalisation}\ccite{Franca2014} + neural networks training \& reverse-engineering
            \item Example algorithms: C-IL$^2$P\ccite{GarcezZ99} and CILP++\ccite{Franca2014}
        \end{itemize}
    \end{block}

    \begin{block}{Meta-Interpretative Learning \cite{MuggletonLPT14}}
        \begin{itemize}
            \item Exploration strategy: bottom-up + top-down
            \item Basic mechanism: \alert{generate and test} + \alert{meta-interpreter} +
            \item Example algorithms: Metagol\ccite{MuggletonLT15}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}%[allowframebreaks]
\frametitle{Key Properties of ILP algorithms}

    \begin{itemize}
        \item \alert{Robustness} w.r.t. noise (a.k.a. wrong examples or background knowledge)
        \bigskip
        \item \alert{Optimality} (a.k.a. minimality of the induced theory)
        \bigskip
        \item Support for \alert{infinite domains}
        \bigskip
        \item Support for \alert{recursion} in induced theories
        \bigskip
        \item Support for \alert{predicate invention}\ccite{Buntine88}
    \end{itemize}
\end{frame}

\begin{frame}%[allowframebreaks]
\frametitle{Predicate ... Invention?!}
\label{slide:pi}

    \begin{block}{Insight}
        \begin{itemize}
            \item \textbf{What:} \alert{inventing} \emph{auxiliary} predicate symbols
            %
            \begin{itemize}
                \item different from those used within the background knowledge
            \end{itemize}
            \item \textbf{Why:} \alert{code reuse}, improving readability of induced theories
            \item \textbf{When:} either \alert{during} induction or \alert{after} it
            \item \textbf{How:} multiple strategies (see below)
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}{Predicate Invention by Grouping Common Literals}

    \begin{block}{Theory without predicate invention}%\scriptsize
        \begin{itemize}
            \item \texttt{a(A, B) :- b1(A), \alert{c(A, B, C), d(C)}.}
            \item \texttt{a(A, B) :- b2(A), \alert{c(A, B, C), d(C)}.}
        \end{itemize}
    \end{block}

    \begin{exampleblock}{Theory with predicate invention}
        \begin{itemize}
            \item \texttt{a(A, B) :- b1(A), invented(A, B, C).}
            \item \texttt{a(A, B) :- b2(A), invented(A, B, C).}
            \item \texttt{invented(A, B, C) :- \alert{c(A, B, C), d(C)}.}
        \end{itemize}
    \end{exampleblock}

\end{frame}

\begin{frame}{Predicate Invention by Grouping Non-Common Literals}

    \begin{block}{Theory without predicate invention}%\scriptsize
        \begin{itemize}
            \item \texttt{a(A, B) :- \alert{b1(A)}, c(A, B, C), d(C).}
            \item \texttt{a(A, B) :- \alert{b2(A)}, c(A, B, C), d(C).}
        \end{itemize}
    \end{block}
    
    \begin{exampleblock}{Theory with predicate invention}%\scriptsize
        \begin{itemize}
            \item \texttt{f(A, B) :- invented(A), c(A, B, C), d(C).}
            \item \texttt{invented(A) :- \alert{b1(A)}.}
            \item \texttt{invented(A) :- \alert{b2(A)}.}
        \end{itemize}
    \end{exampleblock} 
\end{frame}

\subsection{Meta-Interpretative Learning}

\begin{frame}[allowframebreaks]{MIL through Metagol\ccite{MuggletonLT15}}
    \begin{itemize}
        \item Didactic, yet powerful approach, for ILP
        %
        \begin{itemize}
            \item straightforward support for minimality, infinite domains, recursion, and predicate invention
        \end{itemize}
        
        \bigskip

        \item leveraging on $2^{nd}$-order clauses (i.e. \alert{meta-rules}) as linguistic bias
        %
        \begin{itemize}
            \item intuitive, and expressive means to control the search space
        \end{itemize}
        
        \bigskip

        \item leveraging on a \alert{logic solver} to test hypotheses
        %
        \begin{itemize}
            \item commonly, a meta-interpreter (as it is implemented on top of Prolog)
        \end{itemize}

        \bigskip

        \item most famous implementation is Metagol\ccite{Buntine88}
        %
        \begin{itemize}
            \item coming with a Prolog-based implementation \uurl{https://github.com/metagol/metagol}
        \end{itemize}
    \end{itemize}

    \framebreak

    \begin{block}{Properties of Metagol}
        \begin{itemize}
            \item[$\times$] \alert{Robustness} w.r.t. noise
            \item[$\checkmark$] \alert{Minimality} of the induced theory
            %
            \begin{itemize}
                \item via iteratively deepening depth-first search (IDDFS)
            \end{itemize}
            \item[$\checkmark$] Support for \alert{infinite domains}
            %
            \begin{itemize}
                \item no need to generate all possible variable instantiation
                \item (depending on which logic solver is adopted behind the scenes)
            \end{itemize}
            \item[$\checkmark$] Support for \alert{recursion} in induced theories
            %
            \begin{itemize}
                \item via recursive meta-rules
            \end{itemize}
            \item[$\checkmark$] Support for \alert{predicate invention}
            %
            \begin{itemize}
                \item as the algorithm may `invent' (and use) random predicate symbols along the way
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Formal Metagol (reverse-engineered\ccite{Cropper2020a})}
    \input{listings/metagol.tex}
\end{frame}

\begin{frame}[allowframebreaks]{Metagol Example -- Induce \pl{grand\_parent/2}}
    \prologimport{listings/grandparent-input.pl}
    \framebreak
    \prologimport[basicstyle=\scriptsize\ttfamily]{listings/grandparent-execution.pl}
\end{frame}

\begin{frame}[allowframebreaks]{Metagol Example -- Induce \pl{last/2}}
    \prologimport{listings/last-input.pl}
    \framebreak
    \prologimport[basicstyle=\scriptsize\ttfamily]{listings/last-execution.pl}
\end{frame}

\begin{frame}{Discussion}
    \begin{itemize}
        \item Exploration of the hypotheses space is example-driven
        \item IDDFS is what brings minimality of the induced theory
        \item Induction of recursive clauses is enabled/disabled by meta-rules
        \item Hypothesis testing performed via Prolog (that's the implementation tech.)
        \item Predicate invention wasn't exemplified
        \item The formulation is non-deterministic!
    \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Pluggable aspects\ccite{Nannini2021}}
    \begin{block}{Generalization}
        Turning a specific ($\approx$ ground) rule into a general one ($\approx$ with variables)
    \end{block}
    \begin{exampleblock}{Approach 1 -- Constants to variables}
        \begin{center}\ttfamily\small
            last([1,2], 2) :- tail([1,2], [2]), last([2], 2)

            $\qquad \Downarrow$ {\footnotesize $\{ \texttt{1}/X, \texttt{2}/Y, \texttt{[]}/Z \}$}

            last([X, Y | Z], Y) :- tail([X, Y | Z], [Y | Z]), last([Y | Z], Y)
        \end{center}
    \end{exampleblock}
    \framebreak
    \begin{exampleblock}{Approach 2 -- Constants to variables, except empty lists}
        \begin{center}\ttfamily\small
            last([1,2], 2) :- tail([1,2], [2]), last([2], 2)

            $\qquad \Downarrow$ {\footnotesize $\{ \texttt{1}/X, \texttt{2}/Y \}$}

            last([X, Y], Y) :- tail([X, Y], [Y]), last([Y], Y)
        \end{center}
    \end{exampleblock}
    \begin{exampleblock}{Approach 3 -- Ground terms to variables, except empty lists}
        \begin{center}\ttfamily\small
            last([1,2], 2) :- tail([1,2], [2]), last([2], 2)

            $\qquad \Downarrow$ {\footnotesize $\{ \texttt{[1,2]}/X, \texttt{2}/Y, \texttt{[2]}/Z \}$}

            last(X, Y) :- tail(X, Z), last(Z, Y)
        \end{center}
    \end{exampleblock}

    \framebreak

    \begin{block}{Pruning}
        Removing correct -- yet semantically useless -- clauses
    \end{block}

	\begin{exampleblock}{Example -- Induced \texttt{append/3} with redundant clauses}\scriptsize
        \begin{itemize}
            \item \texttt{append([], A, A).}
            \item \alert{\texttt{append([A | As], [], [A | As]) :- append(As, [], As).}}
            \item \alert{\texttt{append([A | As], B, [A | B]) :- append([], B, B).}}
            \item \texttt{append([A | As], B, [A | Cs]) :- append(As, B, Cs).}
        \end{itemize}
    \end{exampleblock}

    \begin{exampleblock}{Example -- Same theory after Occamistic validation\ccite{Nannini2021}}\scriptsize
        \begin{itemize}
            \item \texttt{append([], A, A).}
            \item \texttt{append([A | As], B, [A | Cs]) :- append(As, B, Cs).}
        \end{itemize}
    \end{exampleblock}

    \framebreak

    \begin{block}{Syntactic Sugar}
        Rewriting clauses to make them more user-friendly
    \end{block}

    \begin{exampleblock}{Example}
        \begin{itemize}
            \item \texttt{last(L, X) :- tail(L, []), head(L, X).}
            \item \texttt{last(L, X) :- tail(L, T), last(T, X).}
            %
            \begin{center}\small
                $\qquad \Downarrow$ {\footnotesize \texttt{tail(L, $t$), head(L, $h$)} $\rightarrow$ \texttt{L} $\equiv$ \texttt{[$h$ | $t$]}}
            \end{center}
            \item \texttt{last([X], X).}
            \item \texttt{last([\_|T], X) :- last(T, X).}
        \end{itemize}
    \end{exampleblock}

    \framebreak

    \begin{block}{Predicate invention}\centering
        see \ref{slide:pi}
    \end{block}

    \framebreak

    \begin{block}{Hypotesis Testing}
        Metagol assumes a logic solver is in place to test induced theories
    \end{block}

    \begin{exampleblock}{Alternatives}
        \begin{itemize}
            \item Prolog
            \item Concurrent LP
            \item Answer set programming (ASP)
            \item[$\vdots$]
        \end{itemize}
    \end{exampleblock}

\end{frame}

\begin{frame}{Issues}
    \begin{itemize}
        \item What about robustness w.r.t. \alert{unnecessary meta-rules}?
        \item Who's in charge of \alert{engineering meta-rules}?
        \item How is ILP \alert{interoperable} with mainstream software platform/languages?
        \item Standard \alert{benchmarks / datasets}?
        \item How can one \alert{tune aspects} of the algorithms?
    \end{itemize}
\end{frame}

%===============================================================================
\section*{}
%===============================================================================

%/////////
\frame{\titlepage}
%/////////

%===============================================================================
\section*{\refname}
%===============================================================================

%%%%
\setbeamertemplate{page number in head/foot}{}
%/////////
% \begin{frame}[c,noframenumbering]{\refname}
\begin{frame}[t,allowframebreaks,noframenumbering]{\refname}
%	\tiny
    \scriptsize
%	\footnotesize
    \nocite{*}
    \bibliographystyle{apalike-AMS}
    \bibliography{ise-lab-ilp}
\end{frame}
%/////////

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
